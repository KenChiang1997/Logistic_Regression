{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Neural Net & Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import torch\n",
    "from   torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "class IrisDataset(Dataset):\n",
    "\n",
    "    # data loading\n",
    "    def __init__(self):\n",
    "        iris = datasets.load_iris()\n",
    "        feature   = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "        target    = pd.DataFrame(iris.target, columns=['target'])\n",
    "        iris_data = pd.concat([target, feature], axis=1)\n",
    "        # Data type change and flatten targets\n",
    "        \n",
    "        self.x = torch.from_numpy(np.array(iris_data)[:, 1:].astype(np.float32))\n",
    "        self.y = torch.from_numpy(np.array(iris_data)[:, [0]].astype(np.longlong).flatten())\n",
    "        self.n_samples = self.x.shape[0]\n",
    "\n",
    "    # working for indexing\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    # return the length of our dataset\n",
    "    def __len__(self):\n",
    "\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "dataset = IrisDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Dataset with Feature Size and Multiple Output Calssification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.1000, 3.5000, 1.4000, 0.2000],\n",
      "        [4.9000, 3.0000, 1.4000, 0.2000],\n",
      "        [4.7000, 3.2000, 1.3000, 0.2000],\n",
      "        [4.6000, 3.1000, 1.5000, 0.2000],\n",
      "        [5.0000, 3.6000, 1.4000, 0.2000]])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[:5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:,][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split and Build Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Split_Size = 0.8\n",
    "train_size = int(Split_Size * len(dataset))\n",
    "test_size  = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "batch_size = 20\n",
    "train_dataloader = DataLoader(dataset=train_dataset , batch_size=batch_size)\n",
    "test_dataloader  = DataLoader(dataset=test_dataset  , batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration (1), X_train Size : torch.Size([20, 4]) , Y_train Size : torch.Size([20])\n",
      "iteration (2), X_train Size : torch.Size([20, 4]) , Y_train Size : torch.Size([20])\n",
      "iteration (3), X_train Size : torch.Size([20, 4]) , Y_train Size : torch.Size([20])\n",
      "iteration (4), X_train Size : torch.Size([20, 4]) , Y_train Size : torch.Size([20])\n",
      "iteration (5), X_train Size : torch.Size([20, 4]) , Y_train Size : torch.Size([20])\n",
      "iteration (6), X_train Size : torch.Size([20, 4]) , Y_train Size : torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "for i,(data,label) in enumerate(train_dataloader):\n",
    "    print(f\"iteration ({i+1})\" + \", X_train Size :\",data.size() , \", Y_train Size :\",label.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Simple Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feed_Forward_Neural_Net(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    input_dim --> regression features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,input_size, hidden_size, num_classes):\n",
    "        super(Feed_Forward_Neural_Net, self).__init__()\n",
    "\n",
    "        self.linear_1  = nn.Linear(input_size,hidden_size)  \n",
    "        self.relu      = nn.ReLU()\n",
    "        self.linear_2  = nn.Linear(hidden_size,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.linear_1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear_2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class parameters():\n",
    "\n",
    "    def __init__(self) :\n",
    "\n",
    "        # Layer Neuron Setting\n",
    "        self.input_size    = 4\n",
    "        self.hidden_size   = 10\n",
    "        self.num_classes   = 3\n",
    "        \n",
    "        # Epochs and Learning Rate \n",
    "        self.num_epochs    = 20\n",
    "        self.learning_rate = 0.01\n",
    "\n",
    "        # Cross Entropy\n",
    "        self.criterion     = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "parameter = parameters()\n",
    "\n",
    "# adam algorithm\n",
    "model     = Feed_Forward_Neural_Net(parameter.input_size, parameter.hidden_size, parameter.num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=parameter.learning_rate,weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2/20 , loss = 0.8755\n",
      "epoch 4/20 , loss = 0.6234\n",
      "epoch 6/20 , loss = 0.4645\n",
      "epoch 8/20 , loss = 0.3813\n",
      "epoch 10/20 , loss = 0.3230\n",
      "epoch 12/20 , loss = 0.2780\n",
      "epoch 14/20 , loss = 0.2372\n",
      "epoch 16/20 , loss = 0.2007\n",
      "epoch 18/20 , loss = 0.1695\n",
      "epoch 20/20 , loss = 0.1431\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(parameter.num_epochs):\n",
    "\n",
    "    for i, (datas, labels) in enumerate(train_dataloader):\n",
    "        \n",
    "        # init optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward -> backward -> update\n",
    "        outputs = model(datas)\n",
    "        loss    = parameter.criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 2 == 0 :\n",
    "        print(f'epoch {epoch+2}/{parameter.num_epochs} , loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       0.91      1.00      0.95        10\n",
      "           2       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.96      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "outputs            = model(test_dataset[:][0].float())\n",
    "heck , predictions = torch.max(outputs, 1)\n",
    "test_label         = test_dataset[:][1].float()\n",
    "\n",
    "print( classification_report(predictions,test_label) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 96.67 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "\n",
    "    for datas, labels in test_dataloader:\n",
    "\n",
    "        outputs = model(datas.float())\n",
    "        check , predictions = torch.max(outputs, 1)\n",
    "\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'accuracy = {np.round(acc,decimals=2)} %')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cf3df4d71a28079a44c5da4fc89abaccf068750c614e1d3e46e1e6a2e981cfe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
